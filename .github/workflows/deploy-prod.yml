name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write

env:
  AWS_REGION: us-east-1

jobs:
  deploy-backend:
    name: Deploy Backend to Production
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          echo "Checking if EKS cluster exists..."
          if aws eks describe-cluster --name globepay-prod-eks --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "Production cluster found, updating kubeconfig..."
            if aws eks update-kubeconfig --name globepay-prod-eks --region ${{ env.AWS_REGION }}; then
              echo "Kubeconfig updated successfully"
              # Test connectivity to the cluster
              echo "Testing cluster connectivity..."
              if kubectl cluster-info >/dev/null 2>&1; then
                echo "Cluster connectivity test passed"
              else
                echo "WARNING: Cluster connectivity test failed, but continuing with deployment..."
              fi
            else
              echo "Failed to update kubeconfig, but continuing with deployment..."
            fi
          else
            echo "Production cluster globepay-prod-eks not found, skipping kubeconfig update"
          fi
      
      - name: Create backup before deployment
        run: |
          # Trigger database backup
          echo "Creating database snapshot before deployment..."
          if aws rds create-db-snapshot \
            --db-instance-identifier globepay-prod-db \
            --db-snapshot-identifier pre-deploy-$(date +%Y%m%d-%H%M%S); then
            echo "Database snapshot created successfully"
          else
            echo "Backup creation failed, continuing with deployment..."
          fi
      
      - name: Run database migrations
        run: |
          echo "Running database migrations..."
          
          # Install kustomize if not available
          if ! command -v kustomize &> /dev/null; then
            echo "Installing kustomize..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi
          
          # Create temporary secret files with actual values (in a real environment, these would come from GitHub secrets)
          echo "Creating temporary secret files..."
          cd k8s/overlays/prod
          cat > .env.secret.tmp << 'EOF'
          DB_PASSWORD=actual_db_password_here
          JWT_SECRET=actual_jwt_secret_here
          EOF
          
          # Replace the .env.secret file temporarily
          mv .env.secret .env.secret.bak
          mv .env.secret.tmp .env.secret
          
          # Set the image tag to the commit SHA with 'sha-' prefix for migration job
          echo "Setting migration job image tag to sha-${GITHUB_SHA:0:7}"
          kustomize edit set image 907849381252.dkr.ecr.us-east-1.amazonaws.com/globepay-backend=907849381252.dkr.ecr.us-east-1.amazonaws.com/globepay-backend:sha-${GITHUB_SHA:0:7}
          
          # Delete existing migration job to avoid immutable field errors
          echo "Deleting existing migration job if it exists..."
          kubectl delete job database-migration -n globepay-prod --ignore-not-found
          
          # Apply the migration job
          echo "Applying migration job..."
          kubectl apply -k ./
          
          # Wait for the migration job to complete
          echo "Waiting for migration job to complete..."
          kubectl wait --for=condition=complete job/database-migration -n globepay-prod --timeout=300s || {
            echo "Migration job failed or timed out"
            kubectl logs job/database-migration -n globepay-prod || true
            # Restore the original .env.secret file
            mv .env.secret .env.secret.tmp
            mv .env.secret.bak .env.secret
            exit 1
          }
          
          # Restore the original .env.secret file
          mv .env.secret .env.secret.tmp
          mv .env.secret.bak .env.secret
          
          echo "Database migrations completed successfully"
      
      - name: Deploy to ArgoCD (Canary)
        run: |
          # Install kustomize if not available
          if ! command -v kustomize &> /dev/null; then
            echo "Installing kustomize..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi
          
          # Deploy with canary strategy
          echo "Deploying application with commit ${GITHUB_SHA}..."
          
          # Create temporary secret files with actual values (in a real environment, these would come from GitHub secrets)
          echo "Creating temporary secret files..."
          cd k8s/overlays/prod
          cat > .env.secret.tmp << 'EOF'
          DB_PASSWORD=actual_db_password_here
          JWT_SECRET=actual_jwt_secret_here
          EOF
          
          # Replace the .env.secret file temporarily
          mv .env.secret .env.secret.bak
          mv .env.secret.tmp .env.secret
          
          # Use kustomize to set the image tag to the commit SHA with 'sha-' prefix
          echo "Setting image tag to sha-${GITHUB_SHA:0:7}"
          kustomize edit set image 907849381252.dkr.ecr.us-east-1.amazonaws.com/globepay-backend=907849381252.dkr.ecr.us-east-1.amazonaws.com/globepay-backend:sha-${GITHUB_SHA:0:7}
          
          # Go back to root
          cd ../../..
          
          # First, ensure the application exists
          if ! kubectl get application globepay-backend-prod -n argocd >/dev/null 2>&1; then
            echo "Application globepay-backend-prod not found, creating it..."
            kubectl apply -k k8s/argocd/ -n argocd
          fi
          
          # Show current application status
          echo "Current application status:"
          kubectl get application globepay-backend-prod -n argocd -o yaml || echo "Failed to get application status"
          
          # Instead of patching, we'll recreate the application with the correct configuration
          echo "Recreating application with correct configuration..."
          kubectl delete application globepay-backend-prod -n argocd || true
          
          # Wait a moment for deletion
          sleep 5
          
          # Apply the application with the correct image tag
          kubectl apply -f k8s/argocd/application.yaml -n argocd
          
          # Restore the original .env.secret file
          cd k8s/overlays/prod
          mv .env.secret .env.secret.tmp
          mv .env.secret.bak .env.secret
          
          # Wait for the application to be ready
          echo "Waiting for application to be ready..."
          sleep 10
           
          echo "Waiting for application to become healthy..."
          if kubectl wait --for=condition=Healthy \
            application/globepay-backend-prod \
            -n argocd \
            --timeout=20m; then
            echo "Application is healthy"
          else
            echo "Application failed to become healthy within timeout"
            kubectl describe application/globepay-backend-prod -n argocd
            exit 1
          fi
      
      - name: Monitor canary deployment
        run: |
          # Install Argo Rollouts CLI
          curl -sLO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-amd64
          chmod +x kubectl-argo-rollouts-linux-amd64
          sudo mv kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts
          
          # Wait for canary analysis
          kubectl wait --for=condition=Progressing \
            rollout/backend \
            -n globepay-prod \
            --timeout=10m || true
          
          # Check analysis status (non-blocking)
          kubectl argo rollouts get rollout backend -n globepay-prod || echo "Rollout status check completed"
      
      - name: Verify deployment
        run: |
          kubectl rollout status deployment/backend -n globepay-prod --timeout=20m
      
      - name: Run production smoke tests
        run: |
          SERVICE_URL="${{ secrets.PROD_API_URL }}"
          
          if [ -z "$SERVICE_URL" ]; then
            echo "PROD_API_URL not set, skipping smoke tests"
            exit 0
          fi
          
          echo "Running health check against $SERVICE_URL..."
          if curl -f ${SERVICE_URL}/health --max-time 30; then
            echo "Health check passed"
          else
            echo "Health check failed"
            exit 1
          fi
          
          echo "Running metrics check..."
          if curl -f ${SERVICE_URL}/metrics --max-time 30; then
            echo "Metrics check passed"
          else
            echo "Metrics check failed"
            exit 1
          fi
      
      - name: Notify deployment
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: |
            Production Backend Deployment: ${{ job.status }}
            Environment: Production
            Commit: ${{ github.sha }}
            Author: ${{ github.actor }}
          channel: ${{ secrets.SLACK_CHANNEL }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  deploy-frontend:
    name: Deploy Frontend to Production
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          echo "Checking if EKS cluster exists..."
          if aws eks describe-cluster --name globepay-prod-eks --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "Production cluster found, updating kubeconfig..."
            if aws eks update-kubeconfig --name globepay-prod-eks --region ${{ env.AWS_REGION }}; then
              echo "Kubeconfig updated successfully"
              # Test connectivity to the cluster
              echo "Testing cluster connectivity..."
              if kubectl cluster-info >/dev/null 2>&1; then
                echo "Cluster connectivity test passed"
              else
                echo "WARNING: Cluster connectivity test failed, but continuing with deployment..."
              fi
            else
              echo "Failed to update kubeconfig, but continuing with deployment..."
            fi
          else
            echo "Production cluster globepay-prod-eks not found, skipping kubeconfig update"
          fi
      
      - name: Deploy to ArgoCD
        run: |
          # Install kustomize if not available
          if ! command -v kustomize &> /dev/null; then
            echo "Installing kustomize..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi
          
          # Deploy frontend application
          echo "Deploying frontend application with commit ${GITHUB_SHA}..."
          
          # Use kustomize to set the image tag to the commit SHA with 'sha-' prefix
          echo "Setting frontend image tag to sha-${GITHUB_SHA:0:7}"
          cd k8s/overlays/frontend-prod
          kustomize edit set image 907849381252.dkr.ecr.us-east-1.amazonaws.com/globepay-frontend=907849381252.dkr.ecr.us-east-1.amazonaws.com/globepay-frontend:sha-${GITHUB_SHA:0:7}
          
          # Commit the updated kustomization.yaml file
          echo "Committing updated kustomization.yaml..."
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add kustomization.yaml
          git commit -m "Update frontend image tag to sha-${GITHUB_SHA:0:7}"
          git push origin main
          
          # Go back to root
          cd ../../..
          
          # Refresh ArgoCD to pick up the changes
          echo "Refreshing ArgoCD to pick up image tag changes..."
          kubectl annotate application globepay-frontend-prod -n argocd argocd.argoproj.io/refresh=hard --overwrite || true
          
          # Wait for the application to be ready
          echo "Waiting for frontend application to be ready..."
          sleep 10
           
          echo "Waiting for frontend application to become healthy..."
          if kubectl wait --for=condition=Healthy \
            application/globepay-frontend-prod \
            -n argocd \
            --timeout=20m; then
            echo "Frontend application is healthy"
          else
            echo "Frontend application failed to become healthy within timeout"
            kubectl describe application/globepay-frontend-prod -n argocd
            exit 1
          fi
      
      - name: Verify frontend deployment
        run: |
          kubectl rollout status deployment/frontend -n globepay-prod --timeout=15m
      
      - name: Deploy to S3 and CloudFront
        run: |
          echo "Building frontend application..."
          cd frontend
          if npm ci && npm run build; then
            echo "Frontend build completed successfully"
          else
            echo "Frontend build failed, exiting..."
            exit 1
          fi
          
          echo "Deploying to S3..."
          if aws s3 sync dist/ s3://globepay-prod-assets/ --delete; then
            echo "S3 deployment completed successfully"
          else
            echo "S3 deployment failed, exiting..."
            exit 1
          fi
          
          echo "Creating CloudFront invalidation..."
          if [ -n "${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}" ]; then
            if aws cloudfront create-invalidation \
              --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} \
              --paths "/*"; then
              echo "CloudFront invalidation created successfully"
            else
              echo "CloudFront invalidation failed, continuing..."
            fi
          else
            echo "CLOUDFRONT_DISTRIBUTION_ID not set, skipping invalidation"
          fi
      
      - name: Verify frontend accessibility
        run: |
          FRONTEND_URL="${{ secrets.PROD_FRONTEND_URL }}"
          
          if [ -z "$FRONTEND_URL" ]; then
            echo "PROD_FRONTEND_URL not set, skipping frontend accessibility check"
            exit 0
          fi
          
          echo "Verifying frontend accessibility at $FRONTEND_URL..."
          if curl -f ${FRONTEND_URL} --max-time 30; then
            echo "Frontend is accessible"
          else
            echo "Frontend is not accessible"
            exit 1
          fi
      
      - name: Notify deployment
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: |
            Production Frontend Deployment: ${{ job.status }}
            Environment: Production
            Commit: ${{ github.sha }}
            Author: ${{ github.actor }}
          channel: ${{ secrets.SLACK_CHANNEL }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}